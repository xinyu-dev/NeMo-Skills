cluster: local
base_output_dir: /workspace/openmathreasoning-demo
expname: omr-demo
suffix: nim  # Suffix for experiment names


# Input file for the first stage (generate_solutions)
# This should be the output of the problem_generation.py pipeline
input_file: ${base_output_dir}/problem-sdg/step-5-merge-data/all-problems.jsonl
# File with benchmark problems you want decontaminate with
contamination_file: ${base_output_dir}/problem-sdg/step-6-decontamination/contamination-labeled.jsonl
# Number of solutions per each problem from the input file to generate
num_random_seeds_to_generate: 4
# Can define initial dependency for the `generate_solutions` stage to run after
initial_dependency: ${expname}-merge-data-nim

# Define judge model server parameters
generate_kwargs:
  # Model to be used for answer judgement
  model: gpt-4o-mini # meta/llama-3.3-70b-instruct
  # Server type to launch the model
  server_type: openai
  # NIM API endpoint
  server_address: https://api.openai.com/v1 # https://integrate.api.nvidia.com/v1
  # Number of generated solutions per problem to judge
  num_random_seeds: ${num_random_seeds_to_generate}


# Define the full sequence of stages for this mode
pipeline_stages:
  - generate_solutions          # Generate initial solutions
  - fill_majority_answer        # Fill missing ground truth answers if any
  - judge_answers               # Judge correctness based on final answer
  - prepare_for_sft             # Prepare the final dataset for SFT training

# Directory structure configuration
directories:
  step-1-generate-solutions: ${base_output_dir}/solution-sdg-${suffix}/step-1-generate-solutions
  step-2-fill-majority: ${base_output_dir}/solution-sdg-${suffix}/step-2-fill-majority
  step-3-judge-answers: ${base_output_dir}/solution-sdg-${suffix}/step-3-judge-answers
  step-4-prepare-sft: ${base_output_dir}/solution-sdg-${suffix}/step-4-prepare-sft

# Stage-specific configurations
stages:
  generate_solutions:
    output_dir: ${directories.step-1-generate-solutions}
    input_file: ${input_file}
    # Arguments passed inside the generate context string (e.g., ++param=value)
    inline_args: "++inference.tokens_to_generate=8192 ++prompt_config=llama3-instruct/gsm8k "
    # Arguments passed as kwargs to the pipeline function (e.g. generate())
    stage_kwargs:
      model: o4-mini # deepseek-ai/deepseek-r1-distill-qwen-32b
      server_type: openai
      server_address: https://api.openai.com/v1 # https://integrate.api.nvidia.com/v1
      num_random_seeds: ${num_random_seeds_to_generate}

  fill_majority_answer:
    output_dir: ${directories.step-2-fill-majority}
    input_dir: ${directories.step-1-generate-solutions}
    dependencies:
      - generate_solutions

  judge_answers:
    output_dir: ${directories.step-3-judge-answers}
    input_dir: ${directories.step-2-fill-majority}
    dependencies:
      - fill_majority_answer
    stage_kwargs: ${generate_kwargs}

  prepare_for_sft:
    output_dir: ${directories.step-4-prepare-sft}
    input_file: ${directories.step-3-judge-answers}/output-rs*.jsonl
    prompt_config: generic/math
    prompt_template: qwen-instruct
    contamination_file: ${contamination_file}
    dependencies:
      - judge_answers
