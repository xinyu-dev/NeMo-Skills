cluster: local #slurm
base_output_dir: /workspace/openmathreasoning-demo
expname: omr-tir
suffix: omr  # Suffix for experiment names


# Input file for the first stage (generate_solutions)
# This should be the output of the problem_generation.py pipeline
input_file: ${base_output_dir}/problem-sdg/step-5-merge-data/all-problems.jsonl
# File with benchmark problems you want decontaminate with
contamination_file: ${base_output_dir}/problem-sdg/step-6-decontamination/contamination-labeled.jsonl
# Number of solutions per each problem from the input file to generate
num_random_seeds_to_generate: 3 # 16
# Can define initial dependency for the `generate_solutions` stage to run after
initial_dependency: ${expname}-merge-data-qwen-instruct

# Define judge model server parameters
generate_kwargs: &generate_kwargs # insert a tag here to make it easier to tag and less repetitive code
  # Model to be used for answer judgement
  model: gpt-4o # /trt_models/qwen2.5-32b-instruct
  # Server type to launch the model
  server_type: openai # trtllm
  # NIM API endpoint
  server_address: https://api.openai.com/v1 # https://integrate.api.nvidia.com/v1
  # Server parameters
  # server_gpus: 8
  # server_nodes: 1
  # Number of generated solutions per problem to judge
  num_random_seeds: ${num_random_seeds_to_generate}

generate_new_summaries_kwargs:
  <<: *generate_kwargs # push the generate_kwargs specification here
  # Generate 4 new summaries for each solution
  num_random_seeds: 3   

# Define the full sequence of stages for this mode
pipeline_stages:
  - generate_solutions          # Generate initial TIR solutions
  - fill_majority_answer        # Fill missing ground truth answers if any
  - judge_answers               # Judge correctness based on final answer
  - postprocess_tir_generations  # Applies basic filtering and changes code blocks separator tags 
  - generate_new_summaries       # Generate new summaries for reasoning solutions
  - judge_new_summaries          # Judge new summaries for reasoning solutions
  - merge_new_summaries          # Merge new summaries for reasoning solutions
  - prepare_for_sft             # Prepare the final dataset for SFT training

# Directory structure configuration
directories:
  step-1-generate-solutions: "${base_output_dir}/solution-sdg-${suffix}/step-1-generate-solutions"
  step-2-fill-majority: "${base_output_dir}/solution-sdg-${suffix}/step-2-fill-majority"
  step-3-judge-answers: "${base_output_dir}/solution-sdg-${suffix}/step-3-judge-answers"
  step-4-postprocess-tir: "${base_output_dir}/solution-sdg-${suffix}/step-4-postprocess-tir"
  step-5-generate-new-summaries: "${base_output_dir}/solution-sdg-${suffix}/step-5-generate-new-summaries"
  step-6-judge-new-summaries: "${base_output_dir}/solution-sdg-${suffix}/step-6-judge-new-summaries"
  step-7-merge-new-summaries: "${base_output_dir}/solution-sdg-${suffix}/step-7-merge-new-summaries"
  step-8-prepare-sft: "${base_output_dir}/solution-sdg-${suffix}/step-8-prepare-sft"

# Stage-specific configurations
stages:
  generate_solutions:
    output_dir: ${directories.step-1-generate-solutions}
    input_file: ${input_file}
    # Arguments passed inside the generate context string (e.g., ++param=value)
    inline_args: >-
      ++prompt_config=openmath/tir
      ++inference.tokens_to_generate=16384
      ++code_execution=true
      ++server.code_execution.max_code_executions=null
      ++server.code_execution.add_remaining_code_executions=true
      ++total_code_executions_in_prompt='[1, 8]'
      ++override_max_code_executions=true
    # Arguments passed as kwargs to the pipeline function (e.g. generate())
    stage_kwargs:
      model: gpt-4.1 # /trt_models/openmath-nemotron-14b # need a good OpenAI here, otherwise cannot follow the the right instruction to adhere to the special token template
      server_type: openai # trtllm
      # server_gpus: 1
      #server_nodes: 1
      server_address: https://api.openai.com/v1 
      num_random_seeds: ${num_random_seeds_to_generate}
      with_sandbox: true  # Enable sandbox for code execution
    # stage_kwargs:
    #   # you can take our published model from HF and use it as data generator
    #   # https://huggingface.co/nvidia/OpenMath-Nemotron-14B
    #   model: /trt_models/openmath-nemotron-14b
    #   with_sandbox: true
    #   server_type: trtllm
    #   server_gpus: 8
    #   server_nodes: 1
    #   num_random_seeds: ${num_random_seeds_to_generate}
    #   num_chunks: 10  # since data is big, we are parallelizing it 10x (for each seed, so in total 80 jobs are scheduled)
    #   # if your slurm cluster has a mandatory job timeout, you can schedule multiple dependent jobs with
    #   # dependent_jobs: N

  fill_majority_answer:
    output_dir: ${directories.step-2-fill-majority}
    input_dir: ${directories.step-1-generate-solutions}
    dependencies:
      - generate_solutions
    # You can use CPU partition for this script, if available
    # stage_kwargs:
    #   partition: cpu

  judge_answers:
    output_dir: ${directories.step-3-judge-answers}
    input_dir: ${directories.step-2-fill-majority}
    dependencies:
      - fill_majority_answer
    # inline_args: "++prompt_template=qwen-instruct" remove template for openai models. 
    stage_kwargs:
      <<: *generate_kwargs

  postprocess_tir_generations:
    output_dir: ${directories.step-4-postprocess-tir}
    input_dir: ${directories.step-3-judge-answers}
    code_begin: "<tool_call>\n"
    code_end: "</tool_call>\n"
    dependencies:
      - judge_answers
    # You can use CPU partition for this script, if available
    # stage_kwargs:
    #   partition: cpu

  generate_new_summaries:
    input_file: ${directories.step-4-postprocess-tir}/postprocessed_output.jsonl
    output_dir: ${directories.step-5-generate-new-summaries}
    dependencies:
      - judge_answers
    # remove ++prompt_template=qwen-instruct ++inference.temperature=0.7 ++inference.tokens_to_generate=2048 for openai models. 
    inline_args: >- 
      ++prompt_config=/nemo_run/code/recipes/openmathreasoning/prompts/summarize-solution.yaml
    stage_kwargs: ${generate_new_summaries_kwargs}
      
  judge_new_summaries:
    input_dir: ${directories.step-5-generate-new-summaries}
    output_dir: ${directories.step-6-judge-new-summaries}

    dependencies:
      - generate_new_summaries
    inline_args: "++prompt_template=qwen-instruct"
    stage_kwargs: ${generate_new_summaries_kwargs}

  merge_new_summaries:
    reasoning_file: ${directories.step-4-postprocess-tir}/postprocessed_output.jsonl
    summary_dir: ${directories.step-6-judge-new-summaries}
    output_dir: ${directories.step-7-merge-new-summaries}
    
    dependencies:
      - judge_new_summaries

  prepare_for_sft:
    input_file: ${directories.step-7-merge-new-summaries}/output.jsonl
    output_dir: ${directories.step-8-prepare-sft}
    prompt_config: openmath/tir
    prompt_template: openmath-instruct
    contamination_file: ${contamination_file}
    dependencies:
      - merge_new_summaries
    inline_args: >-
      ++filters.remove_matplotlib=true
      ++exclude_optional_keys=false
      ++add_code_execution_counts=true
    # You can use CPU partition for this script, if available
    # stage_kwargs:
    #   partition: cpu
